name: Check URLs with Lychee and Sphinx

on:
  push:
    branches:
      - main
      - develop
  pull_request:
  schedule:
    # Run every day at 3 am UTC
    - cron: "0 3 * * *"

jobs:
  linkChecker:
    runs-on: ubuntu-latest
    steps:

      # Cache Lychee results to avoid hitting rate limits
      - name: Restore lychee cache
        uses: actions/cache@v3
        with:
          path: .lycheecache
          key: cache-lychee-${{ github.sha }}
          restore-keys: cache-lychee-

      # Check URLs with Lychee
      - uses: actions/checkout@v4

      - name: Lychee URL checker
        uses: lycheeverse/lychee-action@v1.8.0
        with:
          # Arguments with file types to check
          args: >-
            --cache
            --no-progress
            --max-cache-age 2d
            --timeout 10
            --max-retries 5
            --skip-missing
            --exclude-loopback
            --exclude https://twitter.com/pybamm_
            --exclude "https://doi\.org|www.sciencedirect\.com/*"
            --exclude https://www.rse.ox.ac.uk
            --accept 200,429
            --exclude-path ./CHANGELOG.md
            --exclude-path ./scripts/update_version.py
            --exclude-path asv.conf.json
            --exclude-path docs/conf.py
            './**/*.rst'
            './**/*.md'
            './**/*.py'
            './**/*.ipynb'
            './**/*.json'
            './**/*.toml'
          # Fail the action on broken links
          fail: true
          jobSummary: true
          format: markdown
        env:
          # To be used in case rate limits are surpassed
          GITHUB_TOKEN: ${{secrets.GITHUB_TOKEN}}

      # Check Sphinx-related links using Sphinx linkcheck builder
      - name: Install Sphinx
        run: |
          pip install -r docs/requirements.txt
      - name: Run Sphinx linkcheck
        run: |
          sphinx-build -b linkcheck docs docs/build/linkcheck

      # Optionally, you can add more steps to handle Sphinx linkcheck results
